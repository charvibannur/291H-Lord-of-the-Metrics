{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# English (CapybaraHermes-2.5-Mistral-7B-GPTQ)"
      ],
      "metadata": {
        "id": "BAW2Kfc9mv-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install optimum\n",
        "! pip install auto-gptq\n",
        "! pip install -U langsmith\n",
        "! pip install langchain-community\n",
        "! pip install optimum\n",
        "! pip install auto-gptq\n",
        "! pip install python-dotenv"
      ],
      "metadata": {
        "id": "mQTK4e5Obv2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "key = userdata.get('HuggingFace')\n",
        "\n",
        "login(key)"
      ],
      "metadata": {
        "id": "Usxh7jrxbyvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_name_or_path = \"TheBloke/CapybaraHermes-2.5-Mistral-7B-GPTQ\"\n",
        "model_english = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
        "                                             device_map=\"auto\",\n",
        "                                             trust_remote_code=False,\n",
        "                                             revision=\"main\")\n",
        "\n",
        "tokenizer_english = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)"
      ],
      "metadata": {
        "id": "oBFe1fLlb1lV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = model_english.config\n",
        "\n",
        "num_layers = config.num_hidden_layers\n",
        "num_attention_heads = config.num_attention_heads\n",
        "\n",
        "print(f\"Number of layers: {num_layers}\")\n",
        "print(f\"Number of attention heads per layer: {num_attention_heads}\")"
      ],
      "metadata": {
        "id": "5Kg0aUEzb4ZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What is the capital of France\"\n",
        "system_message = \"You are a fact database\"\n",
        "prompt_template=f'''<|im_start|>system\n",
        "{system_message}<|im_end|>\n",
        "<|im_start|>user\n",
        "{prompt}<|im_end|>\n",
        "<|im_start|>assistant\n",
        "'''\n",
        "\n",
        "print(\"\\n\\n*** Generate:\")\n",
        "\n",
        "input_ids = tokenizer_english(prompt_template, return_tensors='pt').to('cuda')\n",
        "output = model_english.generate(**input_ids, return_dict_in_generate=True, max_new_tokens=512 ,output_attentions=True)"
      ],
      "metadata": {
        "id": "bcnSxLDcb7ol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_text = tokenizer_english.decode(output.sequences[0], skip_special_tokens=True)\n",
        "print(output_text)"
      ],
      "metadata": {
        "id": "Gh_ngcwydlLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "FSJ6AisvjAea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "lkey = userdata.get('LangSmith')\n",
        "\n",
        "! export LANGCHAIN_TRACING_V2=true\n",
        "! export LANGCHAIN_API_KEY=lkey"
      ],
      "metadata": {
        "id": "ZF2RyC6ijDF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "key = userdata.get('HuggingFace')\n",
        "login(key)"
      ],
      "metadata": {
        "id": "FeVp8ljDjMXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langsmith import Client\n",
        "client = Client()"
      ],
      "metadata": {
        "id": "YrBtxZB_jTb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline"
      ],
      "metadata": {
        "id": "8q58izjCjU3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = pipeline(\"text-generation\", model=model_english, tokenizer=tokenizer_english)"
      ],
      "metadata": {
        "id": "AZ9yVNwejoXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"squad\")"
      ],
      "metadata": {
        "id": "p8bbPmhEj19a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langsmith import evaluate\n",
        "\n",
        "\n",
        "def evaluate_text_generation(model, dataset):\n",
        "    generated_texts = []\n",
        "    expected_texts = []\n",
        "    i = 0\n",
        "    for example in dataset[\"validation\"]:\n",
        "        prompt = example[\"question\"]\n",
        "        expected_answer = example[\"answers\"][\"text\"][0]\n",
        "\n",
        "        generated_text = model(prompt, max_length=100, num_return_sequences=1)[0][\"generated_text\"]\n",
        "        generated_texts.append(generated_text)\n",
        "        expected_texts.append(expected_answer)\n",
        "        i += 1\n",
        "        if i == 2:\n",
        "          break\n",
        "    return {\"generated_texts\": generated_texts, \"expected_texts\": expected_texts}\n"
      ],
      "metadata": {
        "id": "ZQaK9tcRj2zD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['validation'][1]"
      ],
      "metadata": {
        "id": "e7TvKXTDj5mj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_text_generation(llm, dataset)"
      ],
      "metadata": {
        "id": "Bufak46Vj88V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Token-Based Evaluation"
      ],
      "metadata": {
        "id": "I-MZlPGMcayu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caLaHZlebOq6"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ngram_counts(tokens, n):\n",
        "    return Counter(tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1))"
      ],
      "metadata": {
        "id": "_FXl4eIObo34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clipped_ngram_counts(reference, prediction, n):\n",
        "\n",
        "    ref_ngrams = ngram_counts(reference, n)\n",
        "    pred_ngrams = ngram_counts(prediction, n)\n",
        "\n",
        "    clipped_counts = {ngram: min(pred_ngrams[ngram], ref_ngrams[ngram]) for ngram in pred_ngrams}\n",
        "    return sum(clipped_counts.values()), sum(pred_ngrams.values())"
      ],
      "metadata": {
        "id": "mB4_lIVgdzVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def brevity_penalty(reference_tokens, prediction_tokens):\n",
        "    ref_length = len(reference_tokens)\n",
        "    pred_length = len(prediction_tokens)\n",
        "\n",
        "    if pred_length == 0:\n",
        "        return 0\n",
        "    if pred_length > ref_length:\n",
        "        return 1\n",
        "    else:\n",
        "        return math.exp(1 - ref_length / pred_length)"
      ],
      "metadata": {
        "id": "aPCP0Xohd3u8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_bleu(reference, prediction, max_n=4, weights=(0.25, 0.25, 0.25, 0.25)):\n",
        "\n",
        "    precision_scores = []\n",
        "    for n in range(1, max_n+1):\n",
        "        clipped_count, total_count = clipped_ngram_counts(reference, prediction, n)\n",
        "        if total_count == 0:\n",
        "            precision_scores.append(0)\n",
        "        else:\n",
        "            precision_scores.append(clipped_count / total_count)\n",
        "\n",
        "    if all(p == 0 for p in precision_scores):\n",
        "        bleu_score = 0\n",
        "    else:\n",
        "        weighted_log_precisions = [w * math.log(p) for w, p in zip(weights, precision_scores) if p > 0]\n",
        "        bleu_score = math.exp(sum(weighted_log_precisions))\n",
        "\n",
        "    bleu_score *= brevity_penalty(reference, prediction)\n",
        "    return bleu_score\n"
      ],
      "metadata": {
        "id": "14KiShQad_xV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_tbleu(reference, prediction, reference_answer, prediction_answer, max_n=4, weights=(0.25, 0.25, 0.25, 0.25)):\n",
        "\n",
        "    precision_scores = []\n",
        "    for n in range(1, max_n+1):\n",
        "        clipped_count, total_count = clipped_ngram_counts(reference_answer, prediction_answer, n)\n",
        "        if total_count == 0:\n",
        "            precision_scores.append(0)\n",
        "        else:\n",
        "            precision_scores.append(clipped_count / total_count)\n",
        "\n",
        "    if all(p == 0 for p in precision_scores):\n",
        "        bleu_score = 0\n",
        "    else:\n",
        "        weighted_log_precisions = [w * math.log(p) for w, p in zip(weights, precision_scores) if p > 0]\n",
        "        bleu_score = math.exp(sum(weighted_log_precisions))\n",
        "\n",
        "    bleu_score *= brevity_penalty(reference, prediction)\n",
        "    return bleu_score\n"
      ],
      "metadata": {
        "id": "hyynGrKipuVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch"
      ],
      "metadata": {
        "id": "b_mxMoAVdpbs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_bleu(reference_tokens, generated_tokens):\n",
        "\n",
        "    smoothing_function = SmoothingFunction().method1\n",
        "    return sentence_bleu([reference_tokens], generated_tokens, smoothing_function=smoothing_function)"
      ],
      "metadata": {
        "id": "eXMTnMq1dnm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_token_probabilities(model, tokenizer, generated_text, device=\"cpu\"):\n",
        "\n",
        "    model = model.to(device)\n",
        "    inputs = tokenizer(generated_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
        "        logits = outputs.logits\n",
        "\n",
        "    softmax = torch.nn.functional.softmax(logits, dim=-1)\n",
        "    input_ids = inputs[\"input_ids\"][0]\n",
        "\n",
        "    token_probs = []\n",
        "    for i, token_id in enumerate(input_ids):\n",
        "        token_probs.append(softmax[0, i, token_id].item())\n",
        "\n",
        "    return token_probs"
      ],
      "metadata": {
        "id": "uPa6p9lydwgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_perplexity(probabilities):\n",
        "    epsilon = 1e-10\n",
        "    probabilities = np.clip(probabilities, epsilon, 1.0)\n",
        "    log_prob_sum = np.sum(np.log(probabilities))\n",
        "    n = len(probabilities)\n",
        "    return math.exp(-log_prob_sum / n)"
      ],
      "metadata": {
        "id": "j8kwscRJdzy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def combined_metric(reference_tokens, generated_tokens, token_probabilities, alpha=0.5):\n",
        "    if reference_tokens == generated_tokens:\n",
        "        return 1.0\n",
        "\n",
        "    bleu = compute_bleu(reference_tokens, generated_tokens)\n",
        "    perplexity = compute_perplexity(token_probabilities)\n",
        "\n",
        "    normalized_perplexity = 1 / (1 + perplexity)\n",
        "\n",
        "    combined_score = alpha * bleu + (1 - alpha) * normalized_perplexity\n",
        "    return combined_score"
      ],
      "metadata": {
        "id": "Gjfz2WQkd40m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What is the capital of France\"\n",
        "system_message = \"You are a fact database\"\n",
        "prompt_template=f'''<|im_start|>system\n",
        "{system_message}<|im_end|>\n",
        "<|im_start|>user\n",
        "{prompt}<|im_end|>\n",
        "<|im_start|>assistant\n",
        "'''\n",
        "\n",
        "print(\"\\n\\n*** Generate:\")\n",
        "\n",
        "input_ids = tokenizer_english(prompt_template, return_tensors='pt').to('cuda')\n",
        "output = model_english.generate(**input_ids, return_dict_in_generate=True, max_new_tokens=512 ,output_attentions=True)\n",
        "output_text = tokenizer_english.decode(output.sequences[0], skip_special_tokens=True)\n",
        "print(output_text)"
      ],
      "metadata": {
        "id": "JinToukSllzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_token_probabilities(model, tokenizer, generated_text, device=\"cpu\"):\n",
        "    model = model.to(device)\n",
        "    # Tokenize the input text\n",
        "    inputs = tokenizer(generated_text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Remove 'token_type_ids' if it exists\n",
        "    if \"token_type_ids\" in inputs:\n",
        "        del inputs[\"token_type_ids\"]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Generate logits using the model\n",
        "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
        "        logits = outputs.logits\n",
        "\n",
        "    # Apply softmax to calculate probabilities\n",
        "    softmax = torch.nn.functional.softmax(logits, dim=-1)\n",
        "    input_ids = inputs[\"input_ids\"][0]\n",
        "\n",
        "    token_probs = []\n",
        "    for i, token_id in enumerate(input_ids):\n",
        "        token_probs.append(softmax[0, i, token_id].item())\n",
        "\n",
        "    return token_probs\n"
      ],
      "metadata": {
        "id": "42j3n_rWx7Yi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reference = tokenizer_english(\"Shakespeare\")\n",
        "generated = tokenizer_english(\"Shakespeare wrote Romeo and Juliet\")\n",
        "\n",
        "generated_text = \"Shakespeare wrote Romeo and Juliet\""
      ],
      "metadata": {
        "id": "4LiEs75DuuUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probabilities = compute_token_probabilities(model_english, tokenizer_english, generated_text)\n",
        "\n",
        "score = combined_metric(reference, generated, probabilities, alpha=0.7)\n",
        "print(\"Combined Metric Score:\", score)\n"
      ],
      "metadata": {
        "id": "avUcLQjJdW2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reference_answer = \"Denver Broncos\"\n",
        "prediction_answer = \"The Denver Broncos represented the AFC at Super Bowl 50\"\n",
        "reference_tokens = tokenizer.tokenize(reference_answer)\n",
        "prediction_tokens = tokenizer.tokenize(prediction_answer)\n",
        "\n"
      ],
      "metadata": {
        "id": "U_JU87wGeV3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_tokens"
      ],
      "metadata": {
        "id": "ZmY2vIGSfD4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tbleu = compute_bleu(reference_tokens, prediction_tokens, reference_answer, prediction_answer)\n",
        "print(f\"Token-Based Score: {tbleu:.4f}\")"
      ],
      "metadata": {
        "id": "Bwd6AqipfCk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleu = compute_tbleu(reference, generated, reference, generated)\n",
        "print(f\"bleu Score: {bleu:.4f}\")"
      ],
      "metadata": {
        "id": "lUJ9MTolYO8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hindi (flan-t5-base)"
      ],
      "metadata": {
        "id": "gAZfHur1aCaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers torch\n",
        "! pip install transformers huggingface_hub\n",
        "! pip install -U langsmith\n",
        "! pip install langchain-community"
      ],
      "metadata": {
        "id": "xT4POhTAaL9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
      ],
      "metadata": {
        "id": "JfdTU9jypoQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_hindi = AutoTokenizer.from_pretrained(\"rahular/varta-t5\")\n",
        "model_hindi = AutoModelForSeq2SeqLM.from_pretrained(\"rahular/varta-t5\")"
      ],
      "metadata": {
        "id": "hQ7w_Wh3jgTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_hindi.to(device)"
      ],
      "metadata": {
        "id": "eyrIbKAkaVqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"Answer in hindi. फ्रांस की राजधानी क्या है?\"\n",
        "input_ids = tokenizer_hindi(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "\n",
        "outputs = model_hindi.generate(input_ids)\n",
        "print(tokenizer_hindi.decode(outputs[0]))"
      ],
      "metadata": {
        "id": "FzDuIjL_aXjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "9TRnQ7dSc7G5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langsmith import Client\n",
        "client = Client()"
      ],
      "metadata": {
        "id": "nBXjh0G5bB_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
        "from datasets import load_dataset\n",
        "llm = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "dataset = load_dataset(\"xquad\", \"xquad.hi\")"
      ],
      "metadata": {
        "id": "8f13GLJ1bi9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langsmith import evaluate\n",
        "\n",
        "def evaluate_text_generation(model, dataset):\n",
        "    generated_texts = []\n",
        "    expected_texts = []\n",
        "\n",
        "    for example in dataset[\"validation\"]:\n",
        "        prompt = \"answer in Hindi \" + example[\"question\"]\n",
        "        expected_answer = example[\"answers\"][\"text\"][0]\n",
        "\n",
        "        generated_text = model(prompt, max_length=100, num_return_sequences=1)[0][\"generated_text\"]\n",
        "\n",
        "        generated_texts.append(generated_text)\n",
        "        expected_texts.append(expected_answer)\n",
        "\n",
        "        break\n",
        "\n",
        "    return {\"generated_texts\": generated_texts, \"expected_texts\": expected_texts}"
      ],
      "metadata": {
        "id": "G7Uu6lIub4SM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['validation'][0]"
      ],
      "metadata": {
        "id": "ksISwrbAcGRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_text_generation(llm, dataset)"
      ],
      "metadata": {
        "id": "CO5EI2hjcvYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Token-Based Evaluation"
      ],
      "metadata": {
        "id": "WrMpIfLOc9Ab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reference_answer = \"308\"\n",
        "prediction_answer = \"1 से 5 तक वर्ड्स ऑफ वर्ड्स\"\n",
        "reference_tokens = tokenizer.tokenize(reference_answer)\n",
        "prediction_tokens = tokenizer.tokenize(prediction_answer)"
      ],
      "metadata": {
        "id": "F3QFSim8dGrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tbleu = compute_tbleu(reference_tokens, prediction_tokens,  reference_answer, prediction_answer)\n",
        "print(f\"Token-Based Score: {tbleu:.4f}\")"
      ],
      "metadata": {
        "id": "RKPqy5hXdKlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleu = compute_bleu(reference_answer, prediction_answer)\n",
        "print(f\"bleu Score: {bleu:.4f}\")"
      ],
      "metadata": {
        "id": "ZFiaTD16dTS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reference_tokens"
      ],
      "metadata": {
        "id": "eN9beRZWpCyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_tokens"
      ],
      "metadata": {
        "id": "bXQcdo_Xo8hr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reference = tokenizer_hindi(\"पेरिस\")\n",
        "generated = tokenizer_hindi(\"फ्रांस की राजधानी पेरिस में एक बार फिर से कोरोना वायरस के नए वेरिएंट के मामले सामने आए हैं\")\n",
        "\n",
        "generated_text = \"फ्रांस की राजधानी पेरिस में एक बार फिर से कोरोना वायरस के नए वेरिएंट के मामले सामने आए हैं\"\n",
        "\n",
        "probabilities = compute_token_probabilities(model_hindi, tokenizer_hindi, generated_text)\n",
        "\n",
        "score = combined_metric(reference, generated, probabilities, alpha=0.5)\n",
        "print(\"Combined Metric Score:\", score)\n"
      ],
      "metadata": {
        "id": "c2eFq8capm4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# German"
      ],
      "metadata": {
        "id": "zOv0ea32gD-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM"
      ],
      "metadata": {
        "id": "YzHir7Jn3ACA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")"
      ],
      "metadata": {
        "id": "qoVEof34248q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "Ksp-MnZj3OTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "id": "6ZD9xp4t3S9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"Answer in German. Was ist die Hauptstadt von Frankreich?\"\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(input_ids)\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "id": "BrkmOgGh3T0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "bWHqoQIEhYIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langsmith import Client\n",
        "client = Client()"
      ],
      "metadata": {
        "id": "9POttUsihavj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
        "from datasets import load_dataset\n",
        "llm = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "dataset = load_dataset(\"xquad\", \"xquad.de\")"
      ],
      "metadata": {
        "id": "PH97QPE5ibV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langsmith import evaluate\n",
        "\n",
        "def evaluate_text_generation(model, dataset):\n",
        "    generated_texts = []\n",
        "    expected_texts = []\n",
        "\n",
        "    for example in dataset[\"validation\"]:\n",
        "        prompt = example[\"question\"]\n",
        "        expected_answer = example[\"answers\"][\"text\"][0]\n",
        "\n",
        "        generated_text = model(prompt, max_length=100, num_return_sequences=1)[0][\"generated_text\"]\n",
        "\n",
        "        generated_texts.append(generated_text)\n",
        "        expected_texts.append(expected_answer)\n",
        "\n",
        "        break\n",
        "\n",
        "    return {\"generated_texts\": generated_texts, \"expected_texts\": expected_texts}"
      ],
      "metadata": {
        "id": "o76FO-hdi0aw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['validation'][0]"
      ],
      "metadata": {
        "id": "yCsd-NDJi9Zp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_text_generation(llm, dataset)"
      ],
      "metadata": {
        "id": "HiqTYJK7jAdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "jEihBSUWhbTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reference_answer = \"Die Verteidigung der Panthers gab nur 308 Punkte ab und belegte\"\n",
        "prediction_answer = \"Die Verteidigung der Panthers gab\"\n",
        "reference_tokens = tokenizer.tokenize(reference_answer)\n",
        "prediction_tokens = tokenizer.tokenize(prediction_answer)"
      ],
      "metadata": {
        "id": "TtuM3i2_hfgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tbleu = compute_bleu(reference_tokens, prediction_tokens)\n",
        "print(f\"Token-Based Score: {tbleu:.4f}\")"
      ],
      "metadata": {
        "id": "uJkRePaHh2i3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleu = compute_bleu(reference_answer, prediction_answer)\n",
        "print(f\"bleu Score: {bleu:.4f}\")"
      ],
      "metadata": {
        "id": "0E3XuVxDh5Gp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}